{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "%matplotlib inline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CoreNLP web server : java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "properties={\n",
    "  'annotators': 'tokenize,ssplit,pos,depparse,parse,mention,openie,ner',\n",
    "  'outputFormat': 'json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input)-n+1):\n",
    "        output.append(input[i:i+n])\n",
    "    return [' '.join(x) for x in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qn_word(sentence):\n",
    "    qn_words = []\n",
    "    found = re.match( r'(.*)?(what|where|why|how|when|which)(.*)?', sentence)\n",
    "    if found :\n",
    "        if found.group(2) <> None:\n",
    "            qn_words.append(found.group(2))\n",
    "    return qn_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_root(dep_tree):\n",
    "    for dep in dep_tree:\n",
    "        if dep['dep'] == 'ROOT':\n",
    "            return str(dep['dependentGloss']).lower()\n",
    "\n",
    "def get_subj(dep_tree):\n",
    "    for dep in dep_tree:\n",
    "        if dep['dep'] == 'nsubj':\n",
    "            return str(dep['dependentGloss']).lower()\n",
    "\n",
    "def get_obj(dep_tree):\n",
    "    for dep in dep_tree:\n",
    "        if dep['dep'] == 'dobj':\n",
    "            return str(dep['dependentGloss']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_openie_features(sentence):\n",
    "    subject_list = []\n",
    "    object_list = []\n",
    "    rel_list = []\n",
    "    if 'openie' in sentence:\n",
    "        for rel_map in sentence['openie']:\n",
    "            subject_list.append(rel_map['subject'])\n",
    "            object_list.append(rel_map['object'])\n",
    "            rel_list.append(rel_map['relation'])\n",
    "    return (set(object_list), set(rel_list), set(subject_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_features(row):\n",
    "    openie_feature_req = False\n",
    "    question1 = nlp.annotate(str(row['question1']), properties)\n",
    "    question2 = nlp.annotate(str(row['question2']), properties)\n",
    "    return_obj = []\n",
    "    qn1_ner_list = []\n",
    "    qn2_ner_list = []\n",
    "    qn1_lemma_list = []\n",
    "    qn2_lemma_list = []\n",
    "    if type(question1) == dict and type(question1) == dict:\n",
    "        if 'sentences' in question1 and 'sentences' in question2 and len(question1['sentences']) <> 0:\n",
    "            for token in question1['sentences'][0]['tokens']:\n",
    "                qn1_lemma_list.append(token['lemma'].lower())\n",
    "                if token['ner'] <> 'O':\n",
    "                    qn1_ner_list.append(token['originalText'].lower())\n",
    "            for token in question1['sentences'][0]['tokens']:\n",
    "                qn2_lemma_list.append(token['lemma'].lower())\n",
    "                if token['ner'] <> 'O':\n",
    "                    qn2_ner_list.append(token['originalText'].lower())\n",
    "            \n",
    "            #ner_overlap        \n",
    "            return_obj.append(len(set(qn1_ner_list).intersection(set(qn2_ner_list))))\n",
    "    \n",
    "            #lemma_overlap\n",
    "            return_obj.append(len(set(qn1_lemma_list).intersection(set(qn2_lemma_list)))/float(len(qn1_lemma_list) + len(qn2_lemma_list)))\n",
    "    \n",
    "            #ner_q1_count\n",
    "            return_obj.append(len(qn1_ner_list))\n",
    "    \n",
    "            #ner_q2_count\n",
    "            return_obj.append(len(qn2_ner_list))\n",
    "    \n",
    "            #qn_word_overlap\n",
    "            return_obj.append(len(set(get_qn_word(str(row['question1']).lower())).intersection(set(get_qn_word(str(row['question2']).lower())))))\n",
    "        \n",
    "            #bigram_overlap\n",
    "            return_obj.append(len(set(ngrams(str(row['question1']).lower(),2)).intersection(set(ngrams(str(row['question2']).lower(),2)))))\n",
    "        \n",
    "            #trigram_overlap\n",
    "            return_obj.append(len(set(ngrams(str(row['question1']).lower(),3)).intersection(set(ngrams(str(row['question2']).lower(),3)))))\n",
    "        \n",
    "            #root_match\n",
    "            return_obj.append(1 if get_root(question1['sentences'][0]['enhancedPlusPlusDependencies']) == get_root(question2['sentences'][0]['enhancedPlusPlusDependencies']) else 0)\n",
    "        \n",
    "            #subj_match\n",
    "            return_obj.append(1 if get_subj(question1['sentences'][0]['enhancedPlusPlusDependencies']) == get_subj(question2['sentences'][0]['enhancedPlusPlusDependencies']) else 0)\n",
    "        \n",
    "            #dobj_match\n",
    "            return_obj.append(1 if get_obj(question1['sentences'][0]['enhancedPlusPlusDependencies']) == get_obj(question2['sentences'][0]['enhancedPlusPlusDependencies']) else 0)\n",
    "        \n",
    "            if openie_feature_req:\n",
    "                #openie features\n",
    "                if 'openie' in question1['sentences'][0] and 'openie' in question2['sentences'][0]: \n",
    "                    qn1_openie_features = get_openie_features(question1['sentences'][0])\n",
    "                    qn2_openie_features = get_openie_features(question2['sentences'][0])\n",
    "        \n",
    "                    if not qn1_openie_features and not qn2_openie_features:\n",
    "                        #subject_overlap\n",
    "                        return_obj.append(len(set(qn1_openie_features[2]).intersection(set(qn2_openie_features[2]))))\n",
    "                        #relation_overlap\n",
    "                        return_obj.append(len(set(qn1_openie_features[1]).intersection(set(qn2_openie_features[1]))))\n",
    "                        #object_overlap\n",
    "                        return_obj.append(len(set(qn1_openie_features[0]).intersection(set(qn2_openie_features[0]))))\n",
    "                    \n",
    "                else:\n",
    "                    return_obj.append(0)\n",
    "                    return_obj.append(0)\n",
    "                    return_obj.append(0)\n",
    "        else:\n",
    "            if openie_feature_req:\n",
    "                return_obj = [0,0.0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "            else :\n",
    "                return_obj = [0,0.0,0,0,0,0,0,0,0,0]\n",
    "    else:\n",
    "        if openie_feature_req:\n",
    "            return_obj = [0,0.0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        else :\n",
    "            return_obj = [0,0.0,0,0,0,0,0,0,0,0]        \n",
    "    return return_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv')\n",
    "data_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp_features_train = data_train.apply(nlp_features, axis=1, raw=True)\n",
    "nlp_features_train_df = pd.DataFrame(list(nlp_features_train), columns=['ner_overlap','lemma_overlap', \n",
    "                                                                           'ner_q1_count','ner_q2_count','qn_word_overlap',\n",
    "                                                                        'bigram_overlap', 'trigram_overlap',\n",
    "                                                                       'root_match', 'subj_match', 'dobj_match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_features_train_df.to_csv('data/nlp_features_train.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('string indices must be integers', u'occurred at index 124593')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-60ce595182ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp_features_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m nlp_features_test_df = pd.DataFrame(list(nlp_features_test), columns=['ner_overlap','lemma_overlap', \n\u001b[0;32m      3\u001b[0m                                                                            \u001b[1;34m'ner_q1_count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ner_q2_count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'qn_word_overlap'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                                         \u001b[1;34m'bigram_overlap'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trigram_overlap'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                                        'root_match', 'subj_match', 'dobj_match'])\n",
      "\u001b[1;32mC:\\Users\\asus\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\asus\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-d51e90da3f6a>\u001b[0m in \u001b[0;36mnlp_features\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m#root_match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mreturn_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mget_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'enhancedPlusPlusDependencies'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mget_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'enhancedPlusPlusDependencies'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m#subj_match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ('string indices must be integers', u'occurred at index 124593')"
     ]
    }
   ],
   "source": [
    "nlp_features_test = data_test.apply(nlp_features, axis=1, raw=True)\n",
    "nlp_features_test_df = pd.DataFrame(list(nlp_features_test), columns=['ner_overlap','lemma_overlap', \n",
    "                                                                           'ner_q1_count','ner_q2_count','qn_word_overlap',\n",
    "                                                                        'bigram_overlap', 'trigram_overlap',\n",
    "                                                                       'root_match', 'subj_match', 'dobj_match'])\n",
    "nlp_features_test_df.to_csv('data/nlp_features_test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#124593\n",
    "data_train[0:25].apply(nlp_features, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    1\n",
       "6    0\n",
       "7    1\n",
       "8    0\n",
       "9    0\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[5:10]['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
