{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "import gensim\n",
    "from keras.layers import Layer\n",
    "import inspect\n",
    "from keras.layers import Merge\n",
    "from keras.backend import tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('input_data/train.csv')\n",
    "test = pd.read_csv('input_data/test.csv')\n",
    "subm = pd.read_csv('input_data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "full_data = np.concatenate((list_sentences_train, list_sentences_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could be improved by adding test set to vocab\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=max_features)\n",
    "filters='!\"#$%&()+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "tokenizer = Tokenizer(filters=filters)\n",
    "tokenizer.fit_on_texts(list(full_data))\n",
    "#tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('input_data/glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom word vectors\n",
    "X = tokenizer.texts_to_sequences(full_data)\n",
    "inv_index_map = {v: k for k, v in tokenizer.word_index.iteritems()}\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X[i])):\n",
    "        X[i][j] = inv_index_map[X[i][j]]\n",
    "        \n",
    "model = gensim.models.Word2Vec(X, size=100)\n",
    "embeddings_index = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.018556716, 0.38440287)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = Input(shape=(maxlen,))\n",
    "# x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "# x = Bidirectional(LSTM(100, return_sequences=True,dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "# x = GlobalMaxPool1D()(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(100, activation=\"relu\")(x)\n",
    "# #x = BatchNormalization()(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "# x = Dense(6, activation=\"sigmoid\")(x)\n",
    "# model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True,\n",
    "                        dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "import keras.backend as K\n",
    "def loss(y_true, y_pred):\n",
    "     return K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "model.compile(loss=loss, optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143613/143613 [==============================] - 861s 6ms/step - loss: 0.2157 - acc: 0.5537 - val_loss: 0.2185 - val_acc: 0.7794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8e4523f50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def schedule(ind):\n",
    "    a = [0.002,0.003, 0.000]\n",
    "    return a[ind]\n",
    "lr = callbacks.LearningRateScheduler(schedule)\n",
    "model.fit(X_t, y, batch_size=64, epochs=1, validation_split=0.1, callbacks=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 13s 85us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "subm[list_classes] = y_test\n",
    "subm.to_csv('submission/LSTM-base-submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
